{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup\n",
    "import os\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "# path to the model weights file.\n",
    "weights_path = './keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = './all_ctg_100/train/'\n",
    "validation_data_dir = './all_ctg_100/validation/'\n",
    "nb_train_samples = 2400\n",
    "nb_validation_samples = 240\n",
    "nb_epoch = 50\n",
    "nb_category = 24\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "## create model that transforms image to intermediate stage\n",
    "model_load = Sequential()\n",
    "model_load.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "model_load.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_load.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_load.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_load.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_load.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_load.add(ZeroPadding2D((1, 1)))\n",
    "model_load.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_load.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# load the weights of the VGG16 networks\n",
    "# (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "# note: when there is a complete match between your model definition\n",
    "# and your weight savefile, you can simply call model.load_weights(filename)\n",
    "assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_load.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    model_load.layers[k].set_weights(weights)\n",
    "f.close()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 24 classes.\n",
      "Found 240 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "## generate training and validation data (intermediate stage)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_train = model_load.predict_generator(generator, nb_train_samples)\n",
    "np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_validation = model_load.predict_generator(generator, nb_validation_samples)\n",
    "np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load training and validation data\n",
    "## create labels\n",
    "train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "train_labels = np.array([])\n",
    "for i in range(nb_category):\n",
    "    current = np.array([i] * 100).astype(int)\n",
    "    train_labels = np.append(train_labels, current)\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "\n",
    "validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "validation_labels = np.array([])\n",
    "for i in range(nb_category):\n",
    "    current = np.array([i] * 10).astype(int)\n",
    "    validation_labels = np.append(validation_labels, current)\n",
    "validation_labels = to_categorical(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      "2400/2400 [==============================] - 5s - loss: 0.2254 - acc: 0.9543 - val_loss: 0.1623 - val_acc: 0.9597\n",
      "Epoch 2/50\n",
      "2400/2400 [==============================] - 5s - loss: 0.1496 - acc: 0.9601 - val_loss: 0.1376 - val_acc: 0.9609\n",
      "Epoch 3/50\n",
      "2400/2400 [==============================] - 5s - loss: 0.1344 - acc: 0.9624 - val_loss: 0.1242 - val_acc: 0.9639\n",
      "Epoch 4/50\n",
      "2400/2400 [==============================] - 6s - loss: 0.1226 - acc: 0.9638 - val_loss: 0.1263 - val_acc: 0.9627\n",
      "Epoch 5/50\n",
      "2400/2400 [==============================] - 11s - loss: 0.1136 - acc: 0.9651 - val_loss: 0.1145 - val_acc: 0.9656\n",
      "Epoch 6/50\n",
      "2400/2400 [==============================] - 14s - loss: 0.1057 - acc: 0.9668 - val_loss: 0.1189 - val_acc: 0.9642\n",
      "Epoch 7/50\n",
      "2400/2400 [==============================] - 14s - loss: 0.0986 - acc: 0.9682 - val_loss: 0.1145 - val_acc: 0.9655\n",
      "Epoch 8/50\n",
      "2400/2400 [==============================] - 15s - loss: 0.0923 - acc: 0.9700 - val_loss: 0.1192 - val_acc: 0.9641\n",
      "Epoch 9/50\n",
      "2400/2400 [==============================] - 20s - loss: 0.0866 - acc: 0.9713 - val_loss: 0.1106 - val_acc: 0.9661\n",
      "Epoch 10/50\n",
      "2400/2400 [==============================] - 27s - loss: 0.0823 - acc: 0.9726 - val_loss: 0.1115 - val_acc: 0.9667\n",
      "Epoch 11/50\n",
      "2400/2400 [==============================] - 27s - loss: 0.0772 - acc: 0.9731 - val_loss: 0.1206 - val_acc: 0.9653\n",
      "Epoch 12/50\n",
      "2400/2400 [==============================] - 28s - loss: 0.0728 - acc: 0.9740 - val_loss: 0.1217 - val_acc: 0.9651\n",
      "Epoch 13/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0685 - acc: 0.9760 - val_loss: 0.1243 - val_acc: 0.9646\n",
      "Epoch 14/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0655 - acc: 0.9770 - val_loss: 0.1227 - val_acc: 0.9656\n",
      "Epoch 15/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0610 - acc: 0.9785 - val_loss: 0.1229 - val_acc: 0.9668\n",
      "Epoch 16/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0583 - acc: 0.9795 - val_loss: 0.1278 - val_acc: 0.9663\n",
      "Epoch 17/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0558 - acc: 0.9802 - val_loss: 0.1244 - val_acc: 0.9635\n",
      "Epoch 18/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0547 - acc: 0.9803 - val_loss: 0.1305 - val_acc: 0.9639\n",
      "Epoch 19/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0508 - acc: 0.9815 - val_loss: 0.1320 - val_acc: 0.9649\n",
      "Epoch 20/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0479 - acc: 0.9820 - val_loss: 0.1214 - val_acc: 0.9649\n",
      "Epoch 21/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0416 - acc: 0.9844 - val_loss: 0.1418 - val_acc: 0.9637\n",
      "Epoch 22/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0448 - acc: 0.9841 - val_loss: 0.1297 - val_acc: 0.9651\n",
      "Epoch 23/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0414 - acc: 0.9851 - val_loss: 0.1482 - val_acc: 0.9623\n",
      "Epoch 24/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0408 - acc: 0.9851 - val_loss: 0.1455 - val_acc: 0.9639\n",
      "Epoch 25/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0407 - acc: 0.9850 - val_loss: 0.1495 - val_acc: 0.9634\n",
      "Epoch 26/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0352 - acc: 0.9871 - val_loss: 0.1600 - val_acc: 0.9623\n",
      "Epoch 27/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0360 - acc: 0.9868 - val_loss: 0.1463 - val_acc: 0.9628\n",
      "Epoch 28/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0334 - acc: 0.9881 - val_loss: 0.1598 - val_acc: 0.9642\n",
      "Epoch 29/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0324 - acc: 0.9885 - val_loss: 0.1625 - val_acc: 0.9653\n",
      "Epoch 30/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0341 - acc: 0.9872 - val_loss: 0.1661 - val_acc: 0.9623\n",
      "Epoch 31/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0312 - acc: 0.9884 - val_loss: 0.1558 - val_acc: 0.9635\n",
      "Epoch 32/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0304 - acc: 0.9889 - val_loss: 0.1641 - val_acc: 0.9613\n",
      "Epoch 33/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0284 - acc: 0.9895 - val_loss: 0.1788 - val_acc: 0.9637\n",
      "Epoch 34/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0271 - acc: 0.9899 - val_loss: 0.1898 - val_acc: 0.9606\n",
      "Epoch 35/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0277 - acc: 0.9901 - val_loss: 0.1776 - val_acc: 0.9634\n",
      "Epoch 36/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0288 - acc: 0.9902 - val_loss: 0.1824 - val_acc: 0.9606\n",
      "Epoch 37/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0263 - acc: 0.9905 - val_loss: 0.1781 - val_acc: 0.9611\n",
      "Epoch 38/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0281 - acc: 0.9899 - val_loss: 0.1854 - val_acc: 0.9627\n",
      "Epoch 39/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0266 - acc: 0.9910 - val_loss: 0.1920 - val_acc: 0.9590\n",
      "Epoch 40/50\n",
      "2400/2400 [==============================] - 31s - loss: 0.0270 - acc: 0.9907 - val_loss: 0.1973 - val_acc: 0.9582\n",
      "Epoch 41/50\n",
      "2400/2400 [==============================] - 32s - loss: 0.0249 - acc: 0.9909 - val_loss: 0.1717 - val_acc: 0.9623\n",
      "Epoch 42/50\n",
      "2400/2400 [==============================] - 32s - loss: 0.0270 - acc: 0.9904 - val_loss: 0.2002 - val_acc: 0.9611\n",
      "Epoch 43/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0264 - acc: 0.9914 - val_loss: 0.2010 - val_acc: 0.9613\n",
      "Epoch 44/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0242 - acc: 0.9916 - val_loss: 0.1849 - val_acc: 0.9616\n",
      "Epoch 45/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0254 - acc: 0.9908 - val_loss: 0.1994 - val_acc: 0.9604\n",
      "Epoch 46/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0235 - acc: 0.9915 - val_loss: 0.2003 - val_acc: 0.9632\n",
      "Epoch 47/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0254 - acc: 0.9910 - val_loss: 0.2083 - val_acc: 0.9635\n",
      "Epoch 48/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0236 - acc: 0.9917 - val_loss: 0.2024 - val_acc: 0.9630\n",
      "Epoch 49/50\n",
      "2400/2400 [==============================] - 29s - loss: 0.0195 - acc: 0.9932 - val_loss: 0.2101 - val_acc: 0.9615\n",
      "Epoch 50/50\n",
      "2400/2400 [==============================] - 30s - loss: 0.0233 - acc: 0.9925 - val_loss: 0.2252 - val_acc: 0.9602\n"
     ]
    }
   ],
   "source": [
    "## train and save weights\n",
    "model_train = Sequential()\n",
    "model_train.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_train.add(Dense(256, activation='relu'))\n",
    "model_train.add(Dropout(0.5))\n",
    "model_train.add(Dense(nb_category, activation='softmax'))\n",
    "model_train.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_train.fit(train_data, train_labels,\n",
    "          nb_epoch=nb_epoch, batch_size=32,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model_train.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##0 save model_load and model_train\n",
    "with open('model_load.pkl', 'w') as f:\n",
    "    pickle.dump(model_load, f)\n",
    "\n",
    "with open('model_train.pkl', 'w') as f:\n",
    "    pickle.dump(model_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define look-up table\n",
    "lookup = {0:'beach', 1:'bridge', 2:'cavern', 3:'cliff', 4:'dam', 5:'desert', 6:'farmland', 7:'glacier', 8:'island', 9:'lake', 10:'market', 11:'marsh', 12:'monument', 13:'mountain', 14:'ocean', 15:'prairie', 16:'rainforest', 17:'road', 18:'skyscraper', 19:'stadium', 20:'taiga', 21:'tundra', 22:'volcano', 23:'waterfall'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stadium'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a test image and predict\n",
    "im = cv2.resize(cv2.imread('./test_images/IMG_9984.JPG'), (img_width, img_height)).astype(np.float32)\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im_transform = model_load.predict(im*1./255)\n",
    "predict_proba = model_train.predict(im_transform)\n",
    "lookup[np.argmax(predict_proba)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.935137152671814, 'desert'),\n",
       " (0.05370878055691719, 'ocean'),\n",
       " (0.008119584992527962, 'prairie'),\n",
       " (0.0013521823566406965, 'island'),\n",
       " (0.0006508431979455054, 'beach'),\n",
       " (0.00034779563429765403, 'cliff'),\n",
       " (0.00017916649812832475, 'tundra'),\n",
       " (0.00010392873809905723, 'dam'),\n",
       " (7.946159166749567e-05, 'bridge'),\n",
       " (6.385222513927147e-05, 'volcano'),\n",
       " (5.721237539546564e-05, 'skyscraper'),\n",
       " (4.743493263958953e-05, 'mountain'),\n",
       " (3.807663597399369e-05, 'glacier'),\n",
       " (3.705767812789418e-05, 'farmland'),\n",
       " (2.5708570319693536e-05, 'marsh'),\n",
       " (1.4255322639655787e-05, 'road'),\n",
       " (1.2928856449434534e-05, 'rainforest'),\n",
       " (6.956678589631338e-06, 'monument'),\n",
       " (6.72828127790126e-06, 'cavern'),\n",
       " (4.862476544076344e-06, 'lake'),\n",
       " (2.8137317258369876e-06, 'stadium'),\n",
       " (2.2937781523069134e-06, 'taiga'),\n",
       " (8.669925932736078e-07, 'waterfall'),\n",
       " (5.490556631571053e-08, 'market')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load a test image and predict\n",
    "im = cv2.resize(cv2.imread('./test_images/IMG_6183.JPG'), (img_width, img_height)).astype(np.float32)\n",
    "#im = cv2.resize(cv2.imread('./train_images_rename/lake/lake_060.jpg'), (img_width, img_height)).astype(np.float32)\n",
    "im = im.transpose((2,0,1))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im_transform = model_load.predict(im*1./255) # not so sure, may not contain all transformations in a image generator\n",
    "predict_proba = model_train.predict(im_transform)\n",
    "locs = map(lambda x:lookup[x], range(nb_category))\n",
    "proba_order = zip(predict_proba.tolist()[0], locs)\n",
    "sorted(proba_order, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "##2 generate testing data(user image--intermediate stage)\n",
    "test_data_dir = './test_images/'\n",
    "test_data_gen_folder = './test_images/test/' # there must be a subfolder(generator requirement)\n",
    "test_data_gen = os.listdir(test_data_gen_folder)\n",
    "test_data_gen = [test_d for test_d in test_data_gen if ('.jpg' in test_d)|('.JPG' in test_d)]\n",
    "nb_test_images = len(test_data_gen)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "test_transformed = model_load.predict_generator(generator, nb_test_images)\n",
    "\n",
    "##2 predict using image-generator instead of direct loading\n",
    "test_transformed_sg = test_transformed[1,:,:,:]\n",
    "test_transformed_sg = np.expand_dims(test_transformed_sg, axis=0)\n",
    "predict_proba = model_train.predict(test_transformed_sg)\n",
    "locs = map(lambda x:lookup[x], range(nb_category))\n",
    "proba_order = zip(predict_proba.tolist()[0], locs)\n",
    "sorted(proba_order, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "##3 multiple images generate testing data(user image--intermediate stage)\n",
    "test_data_dir = './test_images/'\n",
    "test_data_gen_folder = './test_images/test/' # there must be a subfolder(generator requirement)\n",
    "test_data_gen = os.listdir(test_data_gen_folder)\n",
    "test_data_gen = [test_d for test_d in test_data_gen if ('.jpg' in test_d)|('.JPG' in test_d)]\n",
    "nb_test_images = len(test_data_gen)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "test_transformed = model_load.predict_generator(generator, nb_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4998003264654751, 'rainforest'),\n",
       " (0.35477447509765625, 'tundra'),\n",
       " (0.14030525088313478, 'desert'),\n",
       " (0.004603391978907327, 'glacier'),\n",
       " (0.00024399514951888902, 'ocean'),\n",
       " (0.00019465064691383867, 'waterfall'),\n",
       " (4.465589154281345e-05, 'prairie'),\n",
       " (1.2171853995826107e-05, 'cliff'),\n",
       " (7.1141480794640485e-06, 'mountain'),\n",
       " (6.052084057461826e-06, 'taiga'),\n",
       " (4.8336711921947995e-06, 'cavern'),\n",
       " (1.2718490214592114e-06, 'beach'),\n",
       " (9.364486004488272e-07, 'farmland'),\n",
       " (5.125231423610665e-07, 'lake'),\n",
       " (1.9560242846619613e-07, 'market'),\n",
       " (9.730300295184013e-08, 'road'),\n",
       " (4.2022364706018056e-08, 'volcano'),\n",
       " (1.6827681498026922e-08, 'bridge'),\n",
       " (1.5076273005959598e-08, 'dam'),\n",
       " (7.390213330182865e-09, 'marsh'),\n",
       " (2.289043787200365e-09, 'island'),\n",
       " (2.0338654857510186e-09, 'skyscraper'),\n",
       " (7.537537822366328e-10, 'stadium'),\n",
       " (5.340672452891493e-10, 'monument')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##3 multiple images average predict using image-generator instead of direct loading\n",
    "\n",
    "mp_predict_proba = np.zeros([nb_test_images, nb_category])\n",
    "\n",
    "for i in range(nb_test_images):\n",
    "    test_transformed_sg = test_transformed[i,:,:,:]\n",
    "    test_transformed_sg = np.expand_dims(test_transformed_sg, axis=0)\n",
    "    predict_proba = model_train.predict(test_transformed_sg)\n",
    "    mp_predict_proba[i,:] = predict_proba\n",
    "    \n",
    "avg_predict_proba = mp_predict_proba.mean(axis = 0)\n",
    "locs = map(lambda x:lookup[x], range(nb_category))\n",
    "proba_order = zip(avg_predict_proba.tolist(), locs)\n",
    "sorted(proba_order, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
